---
title: "Big Data Analysis: Exercises"
author: "Xoel Mato Blanco"
output:
  pdf_document: default
  html_notebook: default
---
# Exercise 1
  
Defining the putative eigen vectors as first and second and the matrix as matr:
```{r}
first <- matrix(c(1,-1), nrow=2)
second <- matrix(c(1,1), nrow=2)
matr <-matrix(c(1,2,2,1), ncol=2)
```
  
To find the eigenvalues and vectors of the matrix, we'll use the eigen function:
```{r}
eigens <- eigen(matr)
eigens
```
  
The eigen vectors obtained (columns) are not different than the proposed ones since the ratio between their values is the same, that is, the vectors are only different in their module. An integer multiplication to the obtained eigenvectors shows that they are the same indeed:
```{r}
eigens$vectors/eigens$vectors[1,1]
```
  
The first eigenvector (column 1) correspond to the second proposed vector:
```{r}
second
```
  
While the second eigenvector (column 2) corresponds to the first proposed vector:
```{r}
first
```
  
Eigen values and vectors meet the following equation:  
  
A * v = lambda * v  
  
Which is true for the obtained and the proposed vectors:  
  
* *Obtained (1)*
  
```{r}
print(matr%*%eigens$vectors[,1])
print(eigens$values[1]*eigens$vectors[,1])
```
  
* *Proposed (2)*
  
```{r}
print(matr%*%second)
print(eigens$values[1]*second)
```
  
* *Obtained (2)*
  
```{r}
print(matr%*%eigens$vectors[,2])
print(eigens$values[2]*eigens$vectors[,2])
```
  
* *Proposed (1)*
  
```{r}
print(matr%*%first)
print(eigens$values[2]*first)
```

# Exercise 2

Defining our function, the gradient function and the test point:
```{r}
func <- function(x){
  return(log(x[1]^2+x[2]^2+1))
}

grad <- function(f, x, delta){
    n <- length(x)
    f0 <- f(x)
    df <- vector("numeric", n)
    for (i in seq(1,n)){
        tmp <- x[i]
        x[i] <- x[i] + delta
        df[i] <- (f(x) - f0) / delta
        x[i] <- tmp
    }
    return(df)
}

test_point <- c(1,1)
```
The gradient in such point is:
```{r}
delta <- 1e-06
grad(func, test_point, delta)
```
# Exercise 3
Defining the gradient descent function:
```{r}
gradient_descent <- function(f, x, delta){
    lambda <- 0.01
    while(lambda > delta){
        df <- grad(f,x,delta)
        x2 <- x - lambda * df
        if ((f(x2) >= f(x)) | is.nan(f(x2)) ){
            lambda <- lambda / 2
            next
        } else {
            x <- x2;
            lambda <- 1.1 * lambda
        }
    }
    return(x)
}
```
Computing the minimum for the given starting point and delta:
```{r}
gradient_descent(func, test_point, delta)
```
That point is indeed (0, 0).
